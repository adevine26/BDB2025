---
title: 'Model: Logistic Regression'
author: "Amelie Devine"
date: "2024-12-26"
output: html_document
--- 

## libraries
```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(data.table)
library(pscl)
library(car)
library(caret)
library(pROC)
library(MLmetrics)
```

## read in data
```{r}
final_data <- fread("data/final_data.csv", data.table = FALSE)
```

## factor man/zone
```{r}
final_data <- final_data %>% 
    mutate(pff_manZone = case_when(
             pff_manZone == "Man" ~ 1,
             TRUE ~ 0))
```

## decide to keep actual deltaX, deltaY, distance values or binary indicators
```{r}
final_data <- final_data %>%
    select(-contains("avgDeltaX"), -contains("avgDeltaY"), -contains("avgDistanceSinceLineset"))

# ## binary indicators
# final_data <- final_data %>%
#   select(-contains("xShift"), -contains("yShift"), -contains("distanceShift"))
```

## drop DL
```{r}
final_data <- final_data %>%
    select(-c("DL_xShift", "DL_yShift", "DL_distanceShift"))
```


## drop irrelevant/hurtful columns
```{r}
final_data <- final_data %>% 
    select(-c("event"))
```

## change data types
```{r}
final_data$pff_manZone <- factor(final_data$pff_manZone)
final_data$receiverAlignment <- factor(final_data$receiverAlignment)
final_data$quarter <- factor(final_data$quarter)

final_data <- final_data %>% 
    mutate(across(everything(), ~ replace_na(., 0)))
```


## here split training/testing
```{r}
set.seed(1223)
sample <- sample(c(TRUE, FALSE), nrow(final_data), replace = TRUE, prob = c(0.8,0.2))

train <- final_data[sample, ]
test <- final_data[!sample, ] 
```

# model
```{r}
drop <- c("gamePlayId", "pff_manZone")
variables <- setdiff(names(train), drop) 
formula <- as.formula(paste("pff_manZone ~", paste(variables, collapse = " + ")))

model <- glm(formula, family = "binomial", data = train)

options(scipen=999)
summary(model)
```


## check VIF of model
```{r}
vif_output <- vif(model)
```

## adjust model due to high vif
```{r}
vif_columns_to_remove <- vif_output %>%
    as.data.frame() %>%
    filter(GVIF > 5) %>%
    rownames()

train <- train[, !names(train) %in% vif_columns_to_remove]
test <- test[, !names(test) %in% vif_columns_to_remove]

variables <- setdiff(names(train), drop) 
formula <- as.formula(paste("pff_manZone ~", paste(variables, collapse = " + ")))

model <- glm(formula, family = "binomial", data = train)

options(scipen=999)
summary(model)
```


## only significant vars
```{r}
sig_model <- glm(pff_manZone~yardsToGo+territoryOffense+ball_y+num_CB+CB_depth+LB_depth+S_depth+
                     CB_distanceShift+LB_distanceShift, 
             family = "binomial", data = train)

options(scipen=999)
summary(sig_model)
```

## cut variables down more
```{r}
sig_model2 <- glm(pff_manZone~yardsToGo+territoryOffense+num_CB+CB_depth+LB_depth+S_depth+
                      CB_distanceShift+LB_distanceShift, 
             family = "binomial", data = train)

options(scipen=999)
summary(sig_model2)
```

## stepwise selection
```{r}
step_model <- step(model, direction = "both")
summary(step_model) ## this is actually worse
```


## check R^2
```{r}
pR2(model)["McFadden"] # 0.2645
pR2(step_model)["McFadden"] # 0.2569
pR2(sig_model)["McFadden"] # 0.2540
pR2(sig_model2)["McFadden"] # 0.2531
```


## residuals
```{r}
residuals <- residuals(model, type = "deviance")
plot(residuals)

step_residuals <- residuals(step_model, type = "deviance")
plot(step_residuals) # very similar

sig_residuals <- residuals(sig_model, type = "deviance")
plot(sig_residuals) # all very similar

sig_residuals2 <- residuals(sig_model2, type = "deviance")
plot(sig_residuals2) # all very similar
```


## accuracy & confusion matrix
```{r}
predictions <- predict(model, newdata = test, type = "response")
predictions_class <- ifelse(predictions > 0.5, 1, 0)
cm <- confusionMatrix(factor(predictions_class), factor(test$pff_manZone))
cm # sensitivity 0.7922, specificity 0.7282

step_predictions <- predict(step_model, newdata = test, type = "response")
step_predictions_class <- ifelse(step_predictions > 0.5, 1, 0)
step_cm <- confusionMatrix(factor(step_predictions_class), factor(test$pff_manZone))
step_cm # sensitivity 0.7843, specificity 0.7184

sig_predictions <- predict(sig_model, newdata = test, type = "response")
sig_predictions_class <- ifelse(sig_predictions > 0.5, 1, 0)
sig_cm <- confusionMatrix(factor(sig_predictions_class), factor(test$pff_manZone))
sig_cm # sensitivity 0.7765, specificity 0.7184

sig_predictions2 <- predict(sig_model2, newdata = test, type = "response")
sig_predictions_class2 <- ifelse(sig_predictions2 > 0.5, 1, 0)
sig_cm2 <- confusionMatrix(factor(sig_predictions_class2), factor(test$pff_manZone))
sig_cm2 # sensitivity 0.7882, specificity 0.7282


```

## accuracy 
```{r}
accuracy <- mean(predictions_class == test$pff_manZone) ## 0.7636

step_accuracy <- mean(step_predictions_class == test$pff_manZone) ## 0.7549

sig_accuracy <- mean(sig_predictions_class == test$pff_manZone) ## 0.7505

sig_accuracy2 <- mean(sig_predictions_class2 == test$pff_manZone) ## 0.7614
```

## F1 score 
```{r}
f1 <- cm$byClass[7]

step_f1 <- step_cm$byClass[7]

sig_f1 <- sig_cm$byClass[7]

sig2_f1 <- sig_cm2$byClass[7]
```


## ROC & AUC
```{r}
roc_curve <- roc(test$pff_manZone, predictions)
plot(roc_curve) # curve is alright
auc(roc_curve) # AUC of 0.8211

step_roc_curve <- roc(test$pff_manZone, step_predictions)
plot(step_roc_curve) # curve is alright
auc(step_roc_curve) # 0.8198

sig_roc_curve <- roc(test$pff_manZone, sig_predictions)
plot(sig_roc_curve) # curve is alright
auc(sig_roc_curve) # 0.8183

sig_roc_curve2 <- roc(test$pff_manZone, sig_predictions2)
plot(sig_roc_curve2)
auc(sig_roc_curve2) # 0.8181

plot(
  sig_roc_curve2,
  main = "ROC Curve",
  col = "blue",
  lwd = 2,
  xlab = "False Positive Rate", 
  ylab = "True Positive Rate",
  xlim = c(1, 0)
) ## prettier roc curve
```

## show threshold of 0.5 is best
```{r}
coords(sig_roc_curve2, "best", ret = "threshold")
thresholds <- seq(0, 1, by = 0.1)
metrics <- data.frame(
    Threshold = thresholds,
    Accuracy = sapply(thresholds, 
                      function(thresh) mean(ifelse(sig_predictions2 >= thresh, 1, 0) == test$pff_manZone)))

ggplot(metrics, aes(x = Threshold)) +
    geom_line(aes(y = Accuracy, color = "Accuracy")) +
    labs(title = "Metric Performance Across Thresholds", y = "Metric Value") 
```

## magnitude of coefficients
```{r}
coefficients <- data.frame(
    Variable = names(coef(sig_model2)),
    Estimate = coef(sig_model2),
    SE = sqrt(diag(vcov(sig_model2)))
)

coefficients$LowerCI <- coefficients$Estimate - 1.96 * coefficients$SE
coefficients$UpperCI <- coefficients$Estimate + 1.96 * coefficients$SE

ggplot(coefficients, aes(x = Estimate, y = Variable)) +
    geom_point() +
    geom_errorbarh(aes(xmin = LowerCI, xmax = UpperCI), height = 0.2) +
    labs(title = "Logistic Regression Coefficients",
         x = "Coefficient Estimate",
         y = "Predictor Variable")
```

## aesthetic confusion matrix
```{r}
draw_confusion_matrix <- function(cm) {

  total <- sum(cm$table)
  res <- as.numeric(cm$table)

  # Generate color gradients. Palettes come from RColorBrewer.
  greenPalette <- c("#F7FCF5","#E5F5E0","#C7E9C0","#A1D99B","#74C476","#41AB5D","#238B45","#006D2C","#00441B")
  redPalette <- c("#FFF5F0","#FEE0D2","#FCBBA1","#FC9272","#FB6A4A","#EF3B2C","#CB181D","#A50F15","#67000D")
  getColor <- function (greenOrRed = "green", amount = 0) {
    if (amount == 0)
      return("#FFFFFF")
    palette <- greenPalette
    if (greenOrRed == "red")
      palette <- redPalette
    colorRampPalette(palette)(100)[10 + ceiling(90 * amount / total)]
  }

  # set the basic layout
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  classes = colnames(cm$table)
  rect(150, 430, 240, 370, col=getColor("green", res[1]))
  text(195, 435, classes[1], cex=1.2)
  rect(250, 430, 340, 370, col=getColor("red", res[3]))
  text(295, 435, classes[2], cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col=getColor("red", res[2]))
  rect(250, 305, 340, 365, col=getColor("green", res[4]))
  text(140, 400, classes[1], cex=1.2, srt=90)
  text(140, 335, classes[2], cex=1.2, srt=90)

  # add in the cm results
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

draw_confusion_matrix(cm)

## code from https://stackoverflow.com/a/53235386
```

## save files
```{r}
write.csv(final_data, "data/final_data.csv")
write.csv(test, "data/test.csv")
saveRDS(sig_model2, "data/model.rds")
```

